<span  style="font-family: Simsun,serif; font-size: 17px; ">

[TOC]

### 下载
- 最新版 [github-ik](https://github.com/medcl/elasticsearch-analysis-ik/releases)    [el](https://www.elastic.co/cn/downloads/elasticsearch)
- 5.5.1 目前和教程一样版本
- ik  https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.5.1/elasticsearch-analysis-ik-5.5.1.zip
- el  https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.1.zip

版本要对应; windows下载后 解压 插件 位置 plugins/elasticsearch

接着，重新启动 Elastic，就会自动加载这个新安装的插件。

然后，新建一个 Index，指定需要分词的字段。这一步根据数据结构而异，下面的命令只针对本文。基本上，凡是需要搜索的中文字段，都要单独设置一下。

使用工具 Sense 百度网盘

~~~bat
$ curl -X PUT 'localhost:9200/accounts' -d '
{
  "mappings": {
    "person": {
      "properties": {
        "user": {
          "type": "text",
          "analyzer": "ik_max_word",
          "search_analyzer": "ik_max_word"
        },
        "title": {
          "type": "text",
          "analyzer": "ik_max_word",
          "search_analyzer": "ik_max_word"
        },
        "desc": {
          "type": "text",
          "analyzer": "ik_max_word",
          "search_analyzer": "ik_max_word"
        }
      }
    }
  }
}'
~~~

上面代码中，首先新建一个名称为accounts的 Index，里面有一个名称为person的 Type。person有三个字段。
- user
- title
- desc

这三个字段都是中文，而且类型都是文本（text），所以需要指定中文分词器，不能使用默认的英文分词器。

Elastic 的分词器称为 ==analyzer==。我们对每个字段指定分词器。

==ik_==

~~~
"user": {
  "type": "text",
  "analyzer": "ik_max_word",
  "search_analyzer": "ik_max_word"
}
~~~

</span>